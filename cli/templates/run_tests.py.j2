#!/usr/bin/env python3
"""
Auto-generated test runner for Gradescope.
Generated by TIF Autograder CLI Tool.
Language: {{ config.language }}
"""

import unittest
import subprocess
import sys
import os
import tempfile
from pathlib import Path
from gradescope_utils.autograder_utils.decorators import weight, visibility
from gradescope_utils.autograder_utils.files import check_submitted_files

class TestRunner(unittest.TestCase):
    """Main test class for autograder."""

    def setUp(self):
        """Set up test environment."""
        self.source_dir = Path('/autograder/source')
        self.submission_files = []
        
        # Get list of submitted files
        for file_path in self.source_dir.iterdir():
            if file_path.is_file():
                self.submission_files.append(file_path.name)

{% for question in config.questions %}
    # {{ question.name }}
{% for item in question.marking_items %}
{% set test_name = (question.name.replace(' ', '_') + '_item_' + loop.index|string)|lower %}

    @weight({{ item.total_mark }})
    @visibility('visible')
    def test_{{ test_name }}(self):
        """{{ question.name }} - Item {{ loop.index }}"""
        
{% if item.type == 0 %}
        # File existence check
        target_file = "{{ item.target_file }}"
        self.assertIn(target_file, self.submission_files, 
                     f"File {target_file} not found in submission")
        
        file_path = self.source_dir / target_file
        self.assertTrue(file_path.exists(), 
                       f"File {target_file} does not exist")
        self.assertTrue(file_path.is_file(), 
                       f"{target_file} is not a file")

{% elif item.type == 1 %}
        # Output comparison test
        target_file = "{{ item.target_file }}"
        expected_input = """{{ item.expected_input }}"""
        expected_output = """{{ item.expected_output }}"""
        
        # Check if file exists first
        file_path = self.source_dir / target_file
        self.assertTrue(file_path.exists(), 
                       f"File {target_file} not found")
        
        # Run the program and compare output
        try:
{% if config.language == 'python' %}
            result = subprocess.run(
                [sys.executable, target_file],
                input=expected_input,
                capture_output=True,
                text=True,
                timeout={{ item.time_limit }},
                cwd=self.source_dir
            )
{% endif %}
            
            if result.returncode != 0:
                self.fail(f"Runtime error: {result.stderr}")
            
            actual_output = result.stdout.strip()
            expected_output_clean = expected_output.strip()
            
            self.assertEqual(actual_output, expected_output_clean,
                           f"Output mismatch.\nExpected: {expected_output_clean}\nActual: {actual_output}")
                           
        except subprocess.TimeoutExpired:
            self.fail(f"Program timed out after {{ item.time_limit }} seconds")
        except Exception as e:
            self.fail(f"Error running program: {str(e)}")

{% elif item.type == 2 %}
        # Signature check
        target_file = "{{ item.target_file }}"
        
        file_path = self.source_dir / target_file
        self.assertTrue(file_path.exists(), 
                       f"File {target_file} not found")
        
        try:
            with open(file_path, 'r') as f:
                content = f.read()
            
{% if config.language == 'python' %}
            # Check for function definitions
            self.assertIn('def ', content, 
                         "No function definitions found in the file")
            
            # Additional Python-specific signature checks can be added here
            # For example, checking for specific function names or parameters
            
        except Exception as e:
            self.fail(f"Error reading file {target_file}: {str(e)}")

{% endif %}

{% endfor %}
{% endfor %}

if __name__ == '__main__':
    # Set up the test suite
    suite = unittest.TestLoader().loadTestsFromTestCase(TestRunner)
    
    # Run tests with Gradescope utilities
    from gradescope_utils.autograder_utils.json_test_runner import JSONTestRunner
    
    with open('/autograder/results/results.json', 'w') as f:
        runner = JSONTestRunner(visibility='visible', stream=f)
        runner.run(suite)
